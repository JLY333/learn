import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.utils.tensorboard import SummaryWriter
from torchviz import make_dot
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix
from scipy.stats import kurtosis, skew
import os
import matplotlib.pyplot as plt
import seaborn as sns
from pyvis.network import Network
import warnings
warnings.filterwarnings("ignore")

# ===================== 1. æ ¸å¿ƒå‚æ•°é…ç½® =====================
DATA_PATH = "./JNU_è½´æ‰¿æ•…éšœ_600rpm.csv"
MODEL_SAVE_PATH = "./best_bearing_bp_model.pth"
LOG_SAVE_PATH = "./bearing_bp_logs"
CM_SAVE_PATH = "./confusion_matrix.png"
NETWORK_VIS_PATH = "./bp_network_visual.html"
WEIGHT_VIS_PATH = "./weight_distribution.png"

WINDOW_SIZE = 1024
STEP = 512
INPUT_DIM = 8       
HIDDEN_DIM1 = 128   
HIDDEN_DIM2 = 64    
OUTPUT_DIM = 4      
BATCH_SIZE = 32     
LEARNING_RATE = 1e-3
EPOCHS = 50
PATIENCE = 5        
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ”§ è®­ç»ƒè®¾å¤‡ï¼š{DEVICE}")
print(f"ğŸ”§ PyTorchç‰ˆæœ¬ï¼š{torch.__version__}")

# ===================== 2. è‡ªå®šä¹‰æ•°æ®é›†ç±» =====================
class BearingDataset(Dataset):
    def __init__(self, features, labels):
        self.features = torch.tensor(features, dtype=torch.float32)
        self.labels = torch.tensor(labels, dtype=torch.long)
    
    def __len__(self):
        return len(self.features)
    
    def __getitem__(self, idx):
        if idx < 0 or idx >= len(self.features):
            raise IndexError(f"âŒ ç´¢å¼•{idx}è¶…å‡ºèŒƒå›´ï¼ˆæ€»æ ·æœ¬æ•°ï¼š{len(self.features)}ï¼‰")
        return self.features[idx], self.labels[idx]

# ===================== 3. æ•°æ®é¢„å¤„ç†å‡½æ•° =====================
def preprocess_bearing_data(data_path, window_size, step):
    try:
        df = pd.read_csv(data_path, encoding="utf-8")
        print("âœ… æ•°æ®é›†ç¼–ç ï¼šutf-8")
    except UnicodeDecodeError:
        df = pd.read_csv(data_path, encoding="gbk")
        print("âœ… æ•°æ®é›†ç¼–ç ï¼šgbkï¼ˆWindowsé»˜è®¤ï¼‰")
    except Exception as e:
        raise FileNotFoundError(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥ï¼š{e}")
    
    drop_cols = ["è½¬é€Ÿ(rpm)", "æºæ–‡ä»¶"]
    dropped_cols = [col for col in drop_cols if col in df.columns]
    df = df.drop(columns=dropped_cols)
    print(f"âœ… åˆ é™¤å†—ä½™åˆ—ï¼š{dropped_cols}")
    
    required_cols = ["æŒ¯åŠ¨ä¿¡å·", "æ•…éšœæ ‡ç­¾"]
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        raise ValueError(f"âŒ æ•°æ®é›†ç¼ºå°‘æ ¸å¿ƒåˆ—ï¼š{missing_cols}")
    
    print(f"âœ… åŸå§‹æ•°æ®é›†æ€»è¡Œæ•°ï¼š{len(df)}")
    print(f"âœ… æ•…éšœæ ‡ç­¾åˆ†å¸ƒï¼š{df['æ•…éšœæ ‡ç­¾'].value_counts().sort_index().to_dict()}")
    
    all_features = []
    all_labels = []
    for label in [0, 1, 2, 3]:
        label_data = df[df["æ•…éšœæ ‡ç­¾"] == label]["æŒ¯åŠ¨ä¿¡å·"].values
        if len(label_data) < window_size:
            print(f"âš ï¸ æ ‡ç­¾{label}æ•°æ®é•¿åº¦ä¸è¶³ï¼Œè·³è¿‡")
            continue
        
        for i in range(0, len(label_data) - window_size + 1, step):
            window_signal = label_data[i:i+window_size]
            if isinstance(window_signal[0], str):
                window_signal = np.array([eval(signal) for signal in window_signal])
            else:
                window_signal = np.array(window_signal, dtype=np.float32)
            
            feature = [
                np.mean(window_signal), np.std(window_signal), kurtosis(window_signal), skew(window_signal),
                np.max(np.abs(window_signal)), np.sqrt(np.mean(window_signal**2)),
                np.max(np.abs(window_signal)) / np.sqrt(np.mean(window_signal**2)),
                np.max(np.abs(window_signal)) / np.mean(np.abs(window_signal))
            ]
            all_features.append(feature)
            all_labels.append(label)
    
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(all_features)
    print(f"\nğŸ“Š æ•°æ®é¢„å¤„ç†å®Œæˆï¼š")
    print(f"   åˆ†çª—åæ€»æ ·æœ¬æ•°ï¼š{len(features_scaled)}")
    print(f"   å„æ ‡ç­¾æ ·æœ¬æ•°ï¼š{np.bincount(all_labels)}")
    return features_scaled, all_labels

# ===================== 4. BPç¥ç»ç½‘ç»œæ¨¡å‹ï¼ˆè®°å½•æ¿€æ´»å€¼ï¼‰ =====================
class BearingBPNet(nn.Module):
    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):
        super(BearingBPNet, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim1)
        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)
        self.fc3 = nn.Linear(hidden_dim2, output_dim)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.2)
        self.activations = {}
    
    def forward(self, x):
        x1 = self.relu(self.fc1(x))
        self.activations['hidden1'] = x1.detach().cpu().numpy()
        x1 = self.dropout(x1)
        
        x2 = self.relu(self.fc2(x1))
        self.activations['hidden2'] = x2.detach().cpu().numpy()
        x2 = self.dropout(x2)
        
        x3 = self.fc3(x2)
        self.activations['output'] = x3.detach().cpu().numpy()
        return x3
    
    def reset_activations(self):
        self.activations = {}

# ===================== 5. å¯è§†åŒ–å‡½æ•°åˆé›†ï¼ˆæ ¸å¿ƒä¿®å¤åºåˆ—åŒ–é—®é¢˜ï¼‰ =====================
def visualize_network_structure(model, dummy_input, save_path):
    """
    ä¿®å¤ç‚¹ï¼š
    1. å¼ºåˆ¶å°†numpyç±»å‹è½¬ä¸ºPythonåŸç”Ÿintï¼Œè§£å†³JSONåºåˆ—åŒ–é—®é¢˜
    2. ç®€åŒ–èŠ‚ç‚¹é‡‡æ ·é€»è¾‘ï¼Œé¿å…numpyç±»å‹æ··å…¥
    3. è®¡ç®—å›¾æç¤ºä¼˜åŒ–
    """
    # -------- 1. è®¡ç®—å›¾å¯è§†åŒ–ï¼ˆéå¿…éœ€ï¼Œå¤±è´¥ä»…æç¤ºï¼‰ --------
    try:
        model_cpu = model.to("cpu")
        dummy_input_cpu = dummy_input.to("cpu")
        y = model_cpu(dummy_input_cpu)
        dot = make_dot(y, params=dict(model_cpu.named_parameters()))
        dot.render("bp_computation_graph", format="pdf")
        print(f"âœ… è®¡ç®—å›¾å·²ä¿å­˜ä¸ºï¼šbp_computation_graph.pdf")
        model.to(DEVICE)
    except Exception as e:
        print(f"âš ï¸ è®¡ç®—å›¾ç»˜åˆ¶å¤±è´¥ï¼ˆéå¿…éœ€ï¼Œå¯å¿½ç•¥ï¼‰ï¼š{e}")
        print(f"   ğŸ’¡ è§£å†³æ–¹æ³•ï¼šå®‰è£…Graphvizç³»ç»Ÿåº“å¹¶é…ç½®PATHï¼Œæˆ–å¿½ç•¥è¯¥æç¤º")
    
    # -------- 2. äº¤äº’å¼ç½‘ç»œæ‹“æ‰‘å¯è§†åŒ–ï¼ˆä¿®å¤JSONåºåˆ—åŒ–ï¼‰ --------
    net = Network(height="800px", width="100%", bgcolor="#222222", font_color="white", directed=True)
    net.barnes_hut()
    
    layers = {
        "Input (è¾“å…¥å±‚)": INPUT_DIM,
        "Hidden 1 (éšè—å±‚1)": HIDDEN_DIM1,
        "Hidden 2 (éšè—å±‚2)": HIDDEN_DIM2,
        "Output (è¾“å‡ºå±‚)": OUTPUT_DIM
    }
    layer_colors = ["#00ff00", "#0080ff", "#ff8000", "#ff0000"]
    layer_ids = []
    node_id = 0  # ç”¨PythonåŸç”Ÿintï¼Œé¿å…numpyç±»å‹
    
    # æ·»åŠ ç¥ç»å…ƒèŠ‚ç‚¹ï¼ˆçº¯Python intï¼Œæ— numpyï¼‰
    for i, (layer_name, num_neurons) in enumerate(layers.items()):
        layer_nodes = []
        # é‡‡æ ·å‡å°‘èŠ‚ç‚¹æ•°ï¼ˆç”¨Python randomï¼Œé¿å…numpyï¼‰
        sample_num = min(10, num_neurons)
        # ç”Ÿæˆé‡‡æ ·ç´¢å¼•ï¼ˆPythonåŸç”Ÿlistï¼‰
        sample_indices = list(range(sample_num))
        
        for n in sample_indices:
            neuron_name = f"{layer_name}-{n+1}"
            # èŠ‚ç‚¹IDç”¨åŸç”Ÿint
            net.add_node(node_id, label=neuron_name, color=layer_colors[i], size=15)
            layer_nodes.append(node_id)
            node_id += 1
        layer_ids.append(layer_nodes)
        
        # æ·»åŠ å±‚æ ‡ç­¾ï¼ˆç›´æ¥æŒ‡å®šä½ç½®ï¼ŒåŸç”Ÿintï¼‰
        net.add_node(
            node_id, 
            label=layer_name, 
            color="#ffffff", 
            size=30, 
            shape="box",
            x=int(i*200),  # å¼ºåˆ¶è½¬åŸç”Ÿint
            y=int(-100)    # å¼ºåˆ¶è½¬åŸç”Ÿint
        )
        node_id += 1
    
    # æ·»åŠ å±‚é—´è¿æ¥ï¼ˆçº¯Pythonç±»å‹ï¼Œé¿å…numpyï¼‰
    for i in range(len(layer_ids)-1):
        current_layer = layer_ids[i]
        next_layer = layer_ids[i+1]
        
        # æ‰‹åŠ¨é‡‡æ ·ï¼ˆé¿å…numpy.randomç”Ÿæˆint32ï¼‰
        sample_size_current = min(5, len(current_layer))
        sample_size_next = min(5, len(next_layer))
        # ç”¨Python random.sampleï¼Œè¿”å›åŸç”Ÿintåˆ—è¡¨
        import random
        random.seed(42)  # å›ºå®šç§å­ï¼Œä¿è¯ç»“æœä¸€è‡´
        sample_current = random.sample(current_layer, sample_size_current)
        sample_next = random.sample(next_layer, sample_size_next)
        
        for c_node in sample_current:
            for n_node in sample_next:
                # è¾¹çš„å±æ€§å…¨éƒ¨ç”¨åŸç”Ÿç±»å‹
                net.add_edge(
                    int(c_node), int(n_node),  # å¼ºåˆ¶è½¬åŸç”Ÿint
                    color="#888888", 
                    width=float(0.5)  # å¼ºåˆ¶è½¬åŸç”Ÿfloat
                )
    
    # ä¿å­˜HTMLï¼ˆä¿®å¤åºåˆ—åŒ–ï¼‰
    try:
        net.save_graph(save_path)
        print(f"âœ… äº¤äº’å¼ç½‘ç»œæ‹“æ‰‘å·²ä¿å­˜ä¸ºï¼š{save_path}ï¼ˆç”¨æµè§ˆå™¨æ‰“å¼€æŸ¥çœ‹ï¼‰")
    except Exception as e:
        print(f"âš ï¸ äº¤äº’å¼æ‹“æ‰‘ç”Ÿæˆå¤±è´¥ï¼š{e}")
        print(f"   ğŸ’¡ å·²è·³è¿‡è¯¥å¯è§†åŒ–ï¼Œä¸å½±å“æ¨¡å‹è®­ç»ƒå’Œå…¶ä»–å¯è§†åŒ–åŠŸèƒ½")

def visualize_weight_distribution(model, save_path):
    plt.rcParams["font.sans-serif"] = ["SimHei"]
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    axes = axes.flatten()
    
    params = {
        "fc1_weight": model.fc1.weight.data.cpu().numpy().flatten(),
        "fc1_bias": model.fc1.bias.data.cpu().numpy().flatten(),
        "fc2_weight": model.fc2.weight.data.cpu().numpy().flatten(),
        "fc2_bias": model.fc2.bias.data.cpu().numpy().flatten(),
        "fc3_weight": model.fc3.weight.data.cpu().numpy().flatten(),
        "fc3_bias": model.fc3.bias.data.cpu().numpy().flatten()
    }
    
    for i, (name, data) in enumerate(params.items()):
        axes[i].hist(data, bins=50, alpha=0.7, color="#1f77b4")
        axes[i].set_title(f"{name} åˆ†å¸ƒ", fontsize=12)
        axes[i].set_xlabel("å‚æ•°å€¼", fontsize=10)
        axes[i].set_ylabel("é¢‘æ¬¡", fontsize=10)
        axes[i].grid(alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
    plt.show()
    print(f"âœ… æƒé‡/åç½®åˆ†å¸ƒå·²ä¿å­˜ä¸ºï¼š{save_path}")

def visualize_activations(model, sample_data, save_prefix="./activation_"):
    plt.rcParams["font.sans-serif"] = ["SimHei"]
    model.eval()
    with torch.no_grad():
        _ = model(sample_data.to(DEVICE))
    
    for layer_name, activations in model.activations.items():
        sample_act = activations[0]
        plt.figure(figsize=(8, 4))
        plt.hist(sample_act, bins=30, alpha=0.7, color="#ff7f0e")
        plt.title(f"{layer_name} å±‚ç¥ç»å…ƒæ¿€æ´»å€¼åˆ†å¸ƒ", fontsize=12)
        plt.xlabel("æ¿€æ´»å€¼", fontsize=10)
        plt.ylabel("ç¥ç»å…ƒæ•°é‡", fontsize=10)
        plt.grid(alpha=0.3)
        save_path = f"{save_prefix}{layer_name}.png"
        plt.savefig(save_path, dpi=300)
        plt.show()
        print(f"âœ… {layer_name}å±‚æ¿€æ´»å€¼åˆ†å¸ƒå·²ä¿å­˜ä¸ºï¼š{save_path}")
    model.reset_activations()

def plot_confusion_matrix(true_labels, pred_labels, save_path):
    cm = confusion_matrix(true_labels, pred_labels)
    plt.rcParams["font.sans-serif"] = ["SimHei"]
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
                xticklabels=["æ­£å¸¸", "å†…åœˆæ•…éšœ", "æ»šåŠ¨ä½“æ•…éšœ", "å¤–åœˆæ•…éšœ"],
                yticklabels=["æ­£å¸¸", "å†…åœˆæ•…éšœ", "æ»šåŠ¨ä½“æ•…éšœ", "å¤–åœˆæ•…éšœ"])
    plt.xlabel("é¢„æµ‹æ ‡ç­¾", fontsize=12)
    plt.ylabel("çœŸå®æ ‡ç­¾", fontsize=12)
    plt.title("è½´æ‰¿æ•…éšœè¯Šæ–­æ··æ·†çŸ©é˜µ", fontsize=14)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
    plt.show()
    print(f"ğŸ“Š æ··æ·†çŸ©é˜µå·²ä¿å­˜è‡³ï¼š{save_path}")

# ===================== 6. è®­ç»ƒå‡½æ•° =====================
def train_bp_model(model, train_loader, test_loader, criterion, optimizer, epochs, patience, device):
    model.to(device)
    best_test_acc = 0.0
    patience_counter = 0
    writer = SummaryWriter(LOG_SAVE_PATH)
    
    for epoch in range(epochs):
        model.train()
        train_loss, train_correct, train_total = 0.0, 0, 0
        for features, labels in train_loader:
            features, labels = features.to(device), labels.to(device)
            outputs = model(features)
            loss = criterion(outputs, labels)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item() * features.size(0)
            _, predicted = torch.max(outputs, 1)
            train_total += labels.size(0)
            train_correct += (predicted == labels).sum().item()
        
        model.eval()
        test_loss, test_correct, test_total = 0.0, 0, 0
        with torch.no_grad():
            for features, labels in test_loader:
                features, labels = features.to(device), labels.to(device)
                outputs = model(features)
                loss = criterion(outputs, labels)
                
                test_loss += loss.item() * features.size(0)
                _, predicted = torch.max(outputs, 1)
                test_total += labels.size(0)
                test_correct += (predicted == labels).sum().item()
        
        train_acc = train_correct / train_total
        test_acc = test_correct / test_total
        avg_train_loss = train_loss / train_total
        avg_test_loss = test_loss / test_total
        
        writer.add_scalar("Loss/Train", avg_train_loss, epoch)
        writer.add_scalar("Loss/Test", avg_test_loss, epoch)
        writer.add_scalar("Accuracy/Train", train_acc, epoch)
        writer.add_scalar("Accuracy/Test", test_acc, epoch)
        
        if test_acc > best_test_acc:
            best_test_acc = test_acc
            torch.save(model.state_dict(), MODEL_SAVE_PATH)
            patience_counter = 0
            print(f"ğŸ“Œ Epoch {epoch+1} | æµ‹è¯•å‡†ç¡®ç‡æå‡è‡³ {best_test_acc:.4f} | ä¿å­˜æœ€ä¼˜æ¨¡å‹")
        else:
            patience_counter += 1
            print(f"Epoch {epoch+1}/{epochs} | "
                  f"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.4f} | "
                  f"Test Loss: {avg_test_loss:.4f} | Test Acc: {test_acc:.4f} | "
                  f"æ—©åœè®¡æ•°å™¨ï¼š{patience_counter}/{patience}")
            if patience_counter >= patience:
                print(f"\nâš ï¸ æ—©åœè§¦å‘ï¼šè¿ç»­{patience}è½®æµ‹è¯•å‡†ç¡®ç‡æ— æå‡ï¼Œç»ˆæ­¢è®­ç»ƒ")
                break
    
    writer.close()
    print(f"\nğŸ¯ è®­ç»ƒç»“æŸ | æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡ï¼š{best_test_acc:.4f}")
    return model

# ===================== 7. ä¸»æµç¨‹ =====================
if __name__ == "__main__":
    if not os.path.exists(DATA_PATH):
        print(f"\nâŒ æ‰¾ä¸åˆ°æ•°æ®é›†æ–‡ä»¶ï¼å½“å‰è·¯å¾„ï¼š{DATA_PATH}")
        exit()
    
    print("\n===== æ­¥éª¤1ï¼šæ•°æ®é¢„å¤„ç† =====")
    try:
        features, labels = preprocess_bearing_data(DATA_PATH, WINDOW_SIZE, STEP)
    except Exception as e:
        print(f"âŒ æ•°æ®é¢„å¤„ç†å¤±è´¥ï¼š{e}")
        exit()
    
    print("\n===== æ­¥éª¤2ï¼šåˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›† =====")
    X_train, X_test, y_train, y_test = train_test_split(
        features, labels, test_size=0.3, random_state=42, stratify=labels
    )
    print(f"âœ… è®­ç»ƒé›†æ ·æœ¬æ•°ï¼š{len(X_train)} | æµ‹è¯•é›†æ ·æœ¬æ•°ï¼š{len(X_test)}")
    
    print("\n===== æ­¥éª¤3ï¼šæ„å»ºæ•°æ®åŠ è½½å™¨ =====")
    train_dataset = BearingDataset(X_train, y_train)
    test_dataset = BearingDataset(X_test, y_test)
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)
    
    print("\n===== æ­¥éª¤4ï¼šåˆå§‹åŒ–æ¨¡å‹å¹¶å¯è§†åŒ–ç»“æ„ =====")
    model = BearingBPNet(INPUT_DIM, HIDDEN_DIM1, HIDDEN_DIM2, OUTPUT_DIM)
    dummy_input = torch.randn(1, INPUT_DIM)  # çº¯CPUå¼ é‡ï¼Œé¿å…è®¾å¤‡å†²çª
    visualize_network_structure(model, dummy_input, NETWORK_VIS_PATH)
    print(f"âœ… æ¨¡å‹ç»“æ„ï¼š\n{model}")
    
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)
    
    print("\n===== æ­¥éª¤5ï¼šå¼€å§‹è®­ç»ƒBPç¥ç»ç½‘ç»œ =====")
    trained_model = train_bp_model(
        model, train_loader, test_loader, criterion, optimizer, EPOCHS, PATIENCE, DEVICE
    )
    
    print("\n===== æ­¥éª¤6ï¼šå¯è§†åŒ–æƒé‡/åç½®åˆ†å¸ƒ =====")
    best_model = BearingBPNet(INPUT_DIM, HIDDEN_DIM1, HIDDEN_DIM2, OUTPUT_DIM)
    best_model.load_state_dict(torch.load(MODEL_SAVE_PATH))
    best_model.to(DEVICE)
    visualize_weight_distribution(best_model, WEIGHT_VIS_PATH)
    
    print("\n===== æ­¥éª¤7ï¼šå¯è§†åŒ–ç¥ç»å…ƒæ¿€æ´»å€¼ =====")
    sample_features, _ = next(iter(test_loader))
    visualize_activations(best_model, sample_features[:1], save_prefix="./activation_")
    
    print("\n===== æ­¥éª¤8ï¼šéªŒè¯æœ€ä¼˜æ¨¡å‹ =====")
    best_model.eval()
    final_correct, final_total = 0, 0
    all_preds = []
    all_true = []
    with torch.no_grad():
        for features, labels in test_loader:
            features, labels = features.to(DEVICE), labels.to(DEVICE)
            outputs = best_model(features)
            _, predicted = torch.max(outputs, 1)
            final_total += labels.size(0)
            final_correct += (predicted == labels).sum().item()
            all_preds.extend(predicted.cpu().numpy())
            all_true.extend(labels.cpu().numpy())
    
    final_acc = final_correct / final_total
    print(f"\nâœ… æœ€ä¼˜æ¨¡å‹æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡ï¼š{final_acc:.4f}")
    plot_confusion_matrix(all_true, all_preds, CM_SAVE_PATH)
    
    print("\nğŸ“¢ æ‰€æœ‰å¯è§†åŒ–ç»“æœå·²ä¿å­˜ï¼š")
    print(f"   - äº¤äº’å¼ç½‘ç»œæ‹“æ‰‘ï¼š{NETWORK_VIS_PATH}ï¼ˆè‹¥ç”Ÿæˆå¤±è´¥å¯å¿½ç•¥ï¼‰")
    print(f"   - æƒé‡åˆ†å¸ƒï¼š{WEIGHT_VIS_PATH}")
    print(f"   - æ··æ·†çŸ©é˜µï¼š{CM_SAVE_PATH}")
    print(f"   - æ¿€æ´»å€¼åˆ†å¸ƒï¼šactivation_*.png")
    print(f"   - TensorBoardæ—¥å¿—ï¼š{LOG_SAVE_PATH}")
    print(f"\nå¯åŠ¨TensorBoardå‘½ä»¤ï¼štensorboard --logdir={LOG_SAVE_PATH}")
    print(f"TensorBoardè®¿é—®åœ°å€ï¼šhttp://localhost:6006")